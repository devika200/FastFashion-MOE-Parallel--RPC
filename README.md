# FastFashion-MOE-Parallel--RPC
ðŸ”¬ Experimental PyTorch Mixture-of-Experts (MoE) using RPC. Demonstrates distributed experts, router, and autograd across workers. Can simulate multi-process training on one machine. Currently not supported on Windows (RPC backend limitation).
